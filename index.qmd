---
title: "Exploring Data, Normality, and Smoothing" 
author: "Barry Quinn <br> Digital Dave"
footer: "Advanced Financial Data Analytics"
css: mycssblend.css
logo: qbslogo.png
title-slide-attributes:
  data-background-image: title-slide.png
  data-background-size: cover
  data-background-opacity: "0.5"
embed-resources: true
format: 
  revealjs:
    slide-number: c/t
    scrollable: true
execute:
  echo: true
---


```{r}
#| include: false
library(tidyverse)
library(tidyquant)
library(fpp2)
library(DT)
```

# Exploring Data

## Outline

- package `tsfe` and class datasets
- `ts` objects and `ts` function
- Visual trends and patterns
- Distributions properties of asset returns
- Time series decomposition

## Rethinking visualisation
  - Charts are not meant to be *seen*, they are intended to be *read*
  - They are not just images but **visual arguments**.
  - Data doesn't *speak for itself*.
  - Data visualisations need to be **shown** and **explained**

## `ts` objects and `ts` function

A time series is stored in a `ts` object in R:

 - a list of numbers
 - information about times those numbers were recorded.

## Example

```{r}
x <- c(123,39,78,52,110)
yr <- 2012:2016
knitr::kable(data.frame(Year=yr,Observation=x), booktabs=TRUE)
```

## `ts` objects and `ts` function {.small}

::: columns
::: column
For observations that are more frequent than once per year, add a `frequency` argument.

E.g., monthly data stored as a numerical vector `z`:

```r
y <- ts(z, frequency=12, start=c(2003, 1))
```
:::
::: column

`ts(data, frequency, start)`

#### Type of data 

|frequency|start|example|
|:--:|:--:|:--:|
|Annual|1 |1995|
|Quarterly|4|`c(1995,2)`|
|Monthly|12|`c(1995,9)`|
|Daily|7 or 365.25|  1 or c(1995,234)|
|Weekly| 52.18| c(1995,23)|
|Hourly|24 or 168 or 8,766  | 1|
|Half-hourly|48 or 336 or 17,532 | 1|
:::
:::


## Class package (pre-loaded in Q-RaP) {.small}

::: columns
::: column

`library(tidyverse); library(tidyquant); library(DT)`

`DT` loads **datatable** interactive table visualisation

`tidyquant` loads:

* **tq_transmute** function (for transforming between time frequencies)
* **tq_mutate** function (create return series)

`tidyverse` loads many packages include:

* **ggplot** plotting package
* **dplyr** data wrangling package
:::
::: column
```
library(fpp2)
```
This loads:

* **forecast** package (for forecasting functions)
* **fma** package (for lots of time series data)
* **expsmooth** package (for more time series data)

:::
:::

## Some class data

```{r} 
#| echo: true
MyDeleteItems<-ls() 
rm(list=MyDeleteItems) 
# Good practice to clear all objects before loading data
library(tsfe) # includes class datasets
print(data(package='tsfe'))
```


## Programmatically accessing data from the 
::: columns
::: column
* Download data using `tg_get` 
* Create a monthly series using `tq_transmute`

```{r}
#| echo: true
#| eval: true
ftse<-tq_get("^FTSE",from="2016-01-01")
ftse_m<-tq_transmute(ftse,
                     select = adjusted,
                     mutate_fun = to.monthly)
ftse_m_ts<-ts(ftse_m$adjusted, start=c(2016,1), freq=12)
```
:::
::: column
- `ts` object has plotting functionality.
- Quick visualisation of financial time trends in monthly ftse 100 index price

```{r, echo=TRUE, fig.height=3}
autoplot(tsfe::ftse_m_ts) + theme_tq()
```
:::
:::

## Is a table a good visualisation?

- Quarterly earnings per share data for carnival PLC.

- Using `DT` to create an interactive table

```{r}
#|echo: true

tsfe::ni_hsales %>% datatable()
```


## Rethinking visualisation using `ggplot2` {.small}

::: columns
::: column
* `ggplot2` is based on [The Grammar of Graphics](http://amzn.to/2ef1eWp), the idea that you can build every graph from the same
components: a data set, a coordinate system, and geoms—visual marks that represent data points.

:::
::: column
#### Example: Exchange rate time series

```{r echo=TRUE, eval= T, fig.height=4}
tsfe::usuk_rate %>%  # Data
  ggplot(aes(x=date, y=price )) + # Coordinate system
  geom_line(colour="pink") # geom
```

:::
:::

## Your turn

* Use `tq_get` to download the CBOE VIX Index from `2016-01-01` using the symbol `^VIX`
* create a time series object using the VIX index price.
* Plot this daily VIX Index Price using `autoplot`.

**Hint:** the ts object of daily financial time series does not have a regular frequency to input into `ts()` function, so leave this argument blank.


# Distributions properties of asset returns

## Why normal? {.small}

* This is a plot of a normal distribution with mean equal to zero and variance equal to one.

```{r}
#| label: normalplot 
#| echo: true
#| fig.height: 2
#| fig.align: "center"
require(grDevices) 
require(graphics)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd =1)
                ,lwd=1,colour="red") + 
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  labs(title=expression(y[it] %~% N(0,1)),
       x=expression(y[it])) +
  theme(plot.title = element_text(hjust = 0.5,size=8))
```


## Why normal?

* Named after Carolo Friderico Gavss, the normal (or Gaussian) distribution is the most common used distributional assumption in statistical analysis.

This is because:

* Easy to calculate with
* Common in nature
* Very conservative assumptions

# Importance of simulation

## `the importance of simulation` {.small}

- Simulation of random variables is important in applied statistics for several reasons. 
- First, we use probability models to mimic variation in the world, and the tools of simulation can help us better
understand how this variation plays out.

>Patterns of randomness are notoriously contrary to normal human thinking—our brains don’t seem to be able to do a good job understanding that random swings will be present in the short term but average out in the long run—and in many cases simulation is a big help in training our intuitions about averages and variation. (Gelman et al. 2021)


## `the importance of simulation`


- .Second, we can use simulation to approximate the sampling distribution of data and propagate this to the sampling distribution of statistical estimates and procedures. 

- Third, regression models are not deterministic; they produce probabilistic predictions. 

## `the importance of simulation`

- Simulation is the most convenient and general way to represent uncertainties in forecasts. 

Throughout this course and in our practice, we use simulation for all these reasons; 

- In this lecture we introduce the basic ideas and the tools required to perform simulations in R.
]

##
:::{.discussion}
* Suppose your company is being IPO'd at a starting price of £40.

* You want to know the future price of the stock in 200 days.

* You have been told Monte Carlo simulation can help predict stock market futures

* Can you create a number of possible future price paths and find average price in 200 days?

* How many futures should you create? 

* What assumptions should we make about these futures?
:::


## Monte Carlo Simulation
- The coding for simulation becomes cleaner if we express the steps for a single simulation in an R function.

```{r}
#|label: customfunction
#| echo: true
path_sim <- function(){
  days <- 200
  changes <- rnorm(200,mean=1.001,sd=0.005)
  sample.path <- cumprod(c(40,changes))
  closing.price <- sample.path[days+1] #+1 because we add the opening price
  return(closing.price)
}
```


## `replicate` function
- For simplicity I have *hard-coded* one sample path of our IPO closing price in the previous function.
- We can then use replicate function to call `path_sim` 10000 times

```{r}
number_of_possible_futures=10000
mc.closing <- replicate(number_of_possible_futures,path_sim())
```

## Summarising simulations

- Simulations are a versatile way to summarise a probability model, predictions from a fitted regression or uncertainty about parameters of a fitted model (*the probabilistic equivalent of estimates and standard errors*)
- One useful way to summarise the *location* of the distribution is to use the `median` function in the variation in the distribution is the *median absolute deviation standard deviation (mad sd)*.

## Median statistics

We typically prefer median-based summaries because they are more computationally stable, and we rescale the median-based summary of variation as described above so as to be comparable to the standard deviation, which we already know how to interpret in usual statistical practice.

```{r}
cat("median = ",median(mc.closing),
    "mad sd = ",mad(mc.closing),
    "mean = ",mean(mc.closing),
    "sd = ",sd(mc.closing))
```


## Why normal?

### Practical reasons

-  Processes that produce normal distributions include:
-  **Addition** of many independent random variables
- Lots of process are approximately normal
- Product of small deviations
- logarithms of products
- Logarithms are just magnitudes


## Ontological perspective of why normal {.small}

::: columns
::: column

<iframe width="560" height="315" src="https://www.youtube.com/embed/XTsaZWzVJ4c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

:::
::: column

* Processes which add fluctuations results in dampening
* Damped fluctuations end up Gaussian
* No information left, except mean and variance.
* Can't infer process from distribution!  Most do some more science.
:::
:::

## Epistemological perspective of why normal {.small}

::: columns
::: column
<iframe width="560" height="315" src="https://www.youtube.com/embed/lI9-YgSzsEQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
:::
::: column

* Know only *mean* and *variance*
* Then least suprising and most conservative (*maximum entropy*) distribution is Gaussian.
  * In terms of being conservative, it assumes the least; only the mean and variance. 
* Nature likes maximum entropy distributions.
:::
:::

## Why normal asset returns?
* Financial time series processes considered in this course:
* Return series
* Financial statement information
* Volatility processes
* Extreme events
* Multivariate series

## Why assume normal asset returns? {.small}
* Normality assumption allows for asset returns properties to be tractable.
* Tractable mean and variance
  * They provide information about the long-term return and risk, respectively.
* Tractable symmetrical properties
  * Symmetry has important implications in holding short and long financial positions in risk management.
* Tractable kurtosis properties 
  * Kurtosis is related to volatility forecasting, efficiency in estimation and tests,etc.

## Simple model of asset returns

* To engineer a statistical model of asset returns we need to make some assumptions about the data story or *data generating process*.

$$ \{R_{it}|i-1,\dots N;t=1,\dots,T\} \stackrel{i.i.d}\sim N (m_1,m_2) $$

* A traditional assumption in financial analysis is the simple returns are independently and identically distributed (iid) as normal with a fixed mean $m_1$ and variance $m_2$.


## Why normal asset returns?

* The previous assumption is unrealistic in a number of ways:
1. The lower bound of a simple return is -1, but a normal distribution has no lower bound.
2. Multiperiod simple returns $R_it[k]$ is not normally distributed as it is the *product* of one-period returns.
3. Empirically, asset returns tend  to have *heavy tails* or **positive excess kurtosis**
]


## Another model of asset returns

$$ \{r_{it}|i=1,\dots N ;t=1,\dots,T\} \stackrel{i.i.d}\sim N (\mu,\sigma^2) $$

* Another common assumption is that log returns are iid as normal with mean $\mu$ and variance $\sigma^2$.
* As the sum of a finite number of iid random variables is normal, $r_t[k]$ is also normally distributed.
* There is also no lower bound for $r_t$
* However, lognormal assumption is not consistent with **positive excess kurtosis**.



## Are stock returns distributed normally? {.small}

* The following code loads Glencore Plc asset prices from yahoo finance, converts daily adjusted prices to monthly log returns, and creates a monthly time series object.

::: columns
::: column
### Ugly code

```{r}
#| echo: true
#| eval: true
glen <- tq_get("GLEN.L")
glen_m <- tq_transmute(glen,
                       select = adjusted,
                       mutate_fun = monthlyReturn,
                       type="log",
                       col_rename = "log_return")
glen_m_ts <- ts(glen_m$log_return,
                frequency=12,start=c(2011,5))
```

:::
::: column
### Piping (%>%) code is more readable?
- `literate programming`
```{r,echo=T,eval=T}
glen <- tq_get("GLEN.L")
glen_m <- glen %>%
  tq_transmute(select = adjusted,
               mutate_fun = monthlyReturn,
               type="log",
               col_rename = "log_return")
glen_m_ts <- glen_m$log_return %>%
  ts(frequency=12,start=c(2011,5))
```
:::
:::

## Are stock returns distributed normally?

**Visual Arguments**

* We can use `ggplot` to visualise the empirical distribution, superimposing what the returns would look like if they were normally distributed.

  * **Only two parameters (a mean and a variance) are required to create a hypothetical normal distribution of a returns series**

## ggplot code

```{r}
glen_m %>% 
  ggplot(aes(x=log_return)) +
  geom_density() +
  stat_function(
    fun=dnorm,
    args=list(mean(glen_m$log_return),
              sd=sd(glen_m$log_return)),
    col="red")
```


## Inference from the plot

* **What patterns are revealed?**
* The normal distribution is superimposed over the histogram of the daily equity returns.
* Compared to the normal the distribution of the returns has longer tails and a higher central peak.
* In statistical terms we say the distribution is leptokurtic, or fat-tailed.


## Are stock returns distributed normally? {.small}

##### Quantile-quantile plot
* A quantile-quantile plot is a graphical tool to compare two distributions.

```{r}
glen_m %>%
  ggplot(aes(sample=100*log_return)) +
  stat_qq(distribution = stats::qnorm) +
  stat_qq_line(distribution = stats::qnorm,
               colour="red") +
  labs(title = "Quantile-quantile plot of glencore stock returns")
```

## Inference from plot
* This plot compares the quantiles of a normal distribution (thinner straight line) to the quantiles of the data (thicker scatter plot).
* If the plots exactly overlap then the data is probably normally distributed.
* While the returns and the normal distribution are similar between +12.5% and -12.5%, outside these limits the returns behave non-normally.


## Should we care if asset returns are normal distributed variables?

- In regressions the assumption of normality of model errors is one of the least important
- For the purpose of estimating the regression line it is barely important at all
- Diagnostics of normality of errors is not recommended unless the model is being used to predict individual data points.


## Is normality of errors important? {.small}

- If the distribution of errors is of interest, perhaps because of predictive goals, this should be distinguished from the distribution of the data, $y$.
- **A regression model does not assume or require that predictors are normally distributed**
- Furthermore, the normal distribution on the outcome refers to the regression errors, not the raw data.
- Depending on the structure of the predictors, it is possible for data $y$ to be far from normally distributed even when coming from a linear regression model.
- See Gelman et al., (2020) Chapter 11 for more detail


## Null hypothesis significant testing {.small}

* Null hypothesis significance tests (NHST) are models.
* We assume an underlying data story with distributional properties which then allows us to create p-values based on null hypothesis.
* In practice they are often misused to create 'bright line' acceptance or rejection decision about underlying theoretical questions.
* In applied statistics their misuse is well understood

>Read Bailey, D. H. & Prado, M. L. de. Finance is Not Excused: Why Finance Should Not Flout Basic Principles of Statistics. Ssrn Electron J (2021) doi:10.2139/ssrn.gi3895330.

## A NHST for normality {.small}

* The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed.
* The test statistic is $W$ and the null hypothesis is that the data is normally distributed.
* The test is sensitive to the tails of the distribution and is recommended for small samples.
```{r}
shapiro.test(glen_m$log_return)
```

## Lung-Box test {.small}

- The Lung-Box test is a test of the null hypothesis that the data is independently and identically distributed.
- The test statistic is $Q$ and the null hypothesis is that the data is independently and identically distributed.
- The test is sensitive to the tails of the distribution and is recommended for small samples.
```{r}
Box.test(glen_m$log_return,lag = 5, type="Ljung-Box")
```

## Are stock returns distributed normally? {.small}

### Statistical inference
* Both test give different result!!  
* Assuming the models underpinning the previous NHST are valid, we can reject the Null that Glencore monthly log returns are normally distributed at even the 1% significance level.

### Practical inference
  
* Assuming normality to model the *middle* of the data is probably ok
* Modelly extreme observations may need more distributional tools.


## Heavy Tail Statistical Distributions 

Student's t 
  : very similar to the normal but wider and lower.
  
Stable 
  : generalisation of the normal, stable under addition thus can be used with log returns, can capture excess kurtosis well but has infinite variance which conflicts with finance theory.
  
Scale mixture
  : a combination of a number of normals.


# Visual explorations

## Rethinking time plots

* One **limitation** with time plots is that the simple passage of time is not a good explanatory variable.
* There are occasional exceptions where there is a clear mechanism driving the financial time series.

>Descriptive chronology is not causal explanation - Edward Tufte ( 2015) *The visual display of quantitative information* P37

## Time plot

```{r, echo=TRUE, fig.height=3}
glen_m %>% ggplot(aes(x=date,y=log_return)) + 
  geom_line()
```



## Seasonal plots
```{r}
glen_m_ts %>% ggseasonplot(year.labels=TRUE,
                           year.labels.left=TRUE) + 
  ylab("") +
  ggtitle("Seasonal plot: Glencore returns")
```

## Inference from the plot

* Data plotted against the individual "seasons" in which the data were observed.  (In this case a "season" is a month.)
  * Something like a time plot except that the data from each season are overlapped.
  * Enables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.
  * In R: `ggseasonplot()`

## Polar plot

```{r}
glen_m_ts %>% ggseasonplot(polar=TRUE) + ylab("")
```


## Subseries plots {.small}

```{r}
ggsubseriesplot(glen_m_ts) + ylab("") +
  ggtitle("Subseries plot: Glencore returns")
```

  * Data for each season collected together in time plot as separate time series.
  * Enables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized.
  * In R: `ggsubseriesplot()`

## Seasonal or cyclic?

#### Time series patterns

Trend
  : pattern exists when there is a long-term increase or decrease in the data.

Seasonal
  : pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).

Cyclic
  : pattern exists when data exhibit rises and falls that are **not of fixed period** (duration usually of at least 2 years).


## Time series components

### Differences between seasonal and cyclic patterns:

* seasonal pattern constant length; cyclic pattern variable length
* average length of cycle longer than length of seasonal pattern
* magnitude of cycle more variable than magnitude of seasonal pattern


## Time series patterns


```{r NI house sales, fig.height=2}
ts(tsfe::ni_hsales$`Total Verified Sales`,start = c(2005,1),frequency = 4)->ni_hsales_ts
autoplot(ni_hsales_ts) +
  ggtitle("Northern Ireland Quarter House Sales") +
  xlab("Year") + ylab("Total Verified Sales")
```


## Time series patterns


```{r eps,fig.height=2}
autoplot(carnival_eps_ts) +
  ggtitle("Quarterly EPS for Carnival Plc") +
  xlab("Year") + ylab("")
```

## Time series patterns

```{r fig.height=2}
tsfe::usuk_rate %>%
  ggplot(aes(x=date,y=price)) +
  geom_line(colour="darkgreen") +
  labs(title=" Time Plot of GBP:USD",
       x="",
       y="Value of £1 in Dollars")
```


## Seasonal or cyclic? {.small}

#### Time series patterns
* seasonal pattern constant length; cyclic pattern variable length
* average length of cycle longer than length of seasonal pattern
* magnitude of cycle more variable than magnitude of seasonal pattern


> The timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.



## Lag plots and autocorrelation {.small}

::: columns
::: column

### Example: Earnings per share

```{r, echo=TRUE}
gglagplot(tsfe::carnival_eps_ts)
```
:::
::: column
### Lagged scatterplots

* Each plot shows $y_t$ plotted against $y_{t-k}$ for
different values of $k$.
* The autocorrelations are the correlations associated
with these scatterplots.

:::
:::

## Autocorrelation {.small}

- One of the most important data properties that financial time series models exploits

- **Covariance** and **correlation**: measure extent of **linear relationship** between two variables ($y$ and $X$).

- **Autocovariance** and **autocorrelation**: measure linear relationship between **lagged values** of a time series $y$.

- We measure the relationship between:

* $y_{t}$ and $y_{t-1}$
* $y_{t}$ and $y_{t-2}$
* $y_{t}$ and $y_{t-3}$
* etc.


## Autocorrelation {.small}

- We denote the sample autocovariance at lag $k$ by $c_k$ and the sample autocorrelation at lag $k$ by $r_k$.  Then define

$$c_k = \frac{1}{T}\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y})$$ 

where $\bar{y}$ is the sample mean of the $y_t$.

$$r_{k} = c_k/c_0$$

* $r_1$ indicates how successive values of  $y$  relate to each other
* $r_2$ indicates how  $y$ values two periods apart relate to each other
* $r_k$ is *almost* the same as the sample correlation between $y_t$ and $y_{t-k}$.



## Autocorrelation

Results for first 9 lags for Carnival earnings data:

```{r, echo=FALSE}
epsacf <- matrix(acf(c(tsfe::carnival_eps_ts), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(epsacf) <- paste("$r_",1:9,"$",sep="")
knitr::kable(epsacf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

## Autocorrelation Function (ACF) plot {.small}

Results for first 9 lags for Carnival earnings data:


```{r epsacf}
ggAcf(tsfe::carnival_eps_ts)
```



## Autocorrelation inference {.small}

- $r_{1}$ is positive and high, indicating that successive values of the series are positively correlated.
* $r_{4}$  higher than for the other lags. This is due to **the seasonal pattern in the data**: the peaks tend to be **4 quarters** apart and the troughs tend to be **2 quarters** apart.
* $r_2$ is more negative than for the other lags because troughs tend to be 2 quarters behind peaks.
* Together, the autocorrelations at lags 1, 2, $\dots$, make up the **autocorrelation** or ACF.
* The plot is known as a **correlogram**



## ACF plots Trend and seasonality
- When data have a trend, the autocorrelations for small lags tend to be large and positive.
- When data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e., at multiples of the seasonal frequency)
- When data are trended and seasonal, you see a combination of these effects.

## Any trends?

```{r2}
autoplot(carnival_eps_ts)
```

## Any autcorrelation?

```{r}
ggAcf(carnival_eps_ts)
```

## Discussion {.discussion}

- Time plot shows clear trend and seasonality.

- The same features are reflected in the ACF.

  * The slowly decaying ACF indicates trend.
  * The ACF peaks at lags 4, 8, 12, 16, 20, $\dots$, indicate seasonality of length 4.

## Your turn

* use `tq_get()` to download GLEN.L stock price from yahoo finance
* using this series create a time series object hint **use frequency=1**
* Explore this time series for trends and autocorrelation


## Signal and the noise {.small}

- On important stylised fact of financial time series data is a low $\frac{Signal}{Noise}$ ratio, which changes over time (is dynamic).
- The signal is the *sense* you want your model to capture and predict, the noise is the *nonsense* you do not want your model to capture as it is unpredictable.

>Noise makes trading in financial markets possible, and thus allows us to observe prices in financial assets. [But] noise also causes markets to be somewhat inefficient…. . Most generally, noise makes it very difficult to test either practical or academic theories about the way that financial or economic markets work. We are forced to act largely in the dark
Fisher Black, Noise, Journal of Finance ,41 ,3 (1986) (p.529)


## Example: White noise

```{r}
set.seed(1222)
wn <- ts(rnorm(36))
autoplot(wn)
```



## Example: White noise

```{r,result='asis',echo=FALSE}
wnacf <- matrix(acf(c(wn), lag.max=10,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(wnacf) <- paste("$r_{",1:10,"}$",sep="")
knitr::kable(wnacf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```
- Sample autocorrelations for white noise series

- We expect each autocorrelation to be close to zero.


## Sampling distribution of autocorrelations

Sampling distribution of $r_k$ for white noise data is asymptotically N(0,$1/T$).

* 95% of all $r_k$ for white noise must lie within $\pm 1.96/\sqrt{T}$.
* If this is not the case, the series is probably not WN.
* Common to plot lines at $\pm 1.96/\sqrt{T}$ when plotting ACF.


## Your turn

- You can compute the daily changes in the Google stock price using
```r
dgoog <- diff(goog)
```
- Does `dgoog` look like white noise?


## Financial Data Forecastability

  - Predictability of an event or quantity depends on several factors
  1. How well we understand the factors that contribute to it
  2. How much data are available
  3. Whether the forecast can affect the thing we are trying to forecast

## Prediction and EMH {.small}
  - Crudely, the efficent market hypothesis (EMH) implies that returns from speculative assets are *unforecastable*
  - The overpowering logic of the EMH is:
    - **If returns are forecastable,there should exist a money machine producing unlimited wealth**
- Based on the random walk theory change in price are defined as white noise as follows:
\begin{align*}
p_t=p_{t-1} + a_t \\
p_t-p_{t-1} = a_t \\
\text{where}
\end{align*}

## Prediction and EMH {.small}

  - High quality models identify the signal from the noise in financial data.
  - The **signal** is the regular pattern that is likely to repeat.
  - The **noise** is the irregular pattern which occurrs by chance and unlikely to repeat.
  - Overfitting or data snooping can result in your model capturing both **signal** and **noise**.
  - Overfitted models usually produce poor predictions and inferences.


# Time Series Modelling

## Time Series Modelling
- Financial time series data often exhibit patterns, trends, and fluctuations
- Appropriate modelling and processing techniques are required to extract meaningful insights
- Two commonly used approaches:
  - ARIMA (Autoregressive Integrated Moving Average) modelling
  - Smoothing techniques

## ARIMA Modelling

- Class of statistical models widely used for time series forecasting and analysis
- Combines autoregressive (AR) and moving average (MA) components
- Handles non-stationarity through differencing

## Key Aspects of ARIMA Modelling {.small}

::: incremental
1. Stationarity
   - Assumes time series is stationary (constant mean, variance, and autocorrelation)
   - Differencing is applied to achieve stationarity if data is non-stationary

2. Autocorrelation
   - Captures the data's autocorrelation structure
   - Future values are influenced by past values and/or past errors

3. Model Identification
   - Specified by three parameters: p (AR order), d (differencing degree), q (MA order)
   - Determined through iterative model identification, estimation, and diagnostic checking

4. Forecasting
   - Generates forecasts for future periods once an appropriate ARIMA model is identified and estimated
:::

## Suitability of ARIMA Models

- Capturing underlying patterns and dynamics of time series data
- Handling trends, seasonality, and autocorrelation structures
- Widely used in finance for forecasting stock prices, exchange rates, and economic indicators

## Smoothing Techniques

- Reduce noise or irregularities in time series data
- Reveal the underlying trend or signal
- Apply filters or weighted averages to smooth out fluctuations
- Do not explicitly model the autocorrelation structure

## Standard Smoothing Techniques

1. Moving Averages (Simple, Exponential, Weighted)
2. Savitzky-Golay Filter
3. Lowess (Locally Weighted Scatterplot Smoothing)
4. Kalman Filter

## Suitability of Smoothing Techniques

- Extracting the underlying trend or signal from noisy data
- Preprocessing step before further analysis or visualization
- Focus on denoising the data and revealing the underlying trend or signal

## Choosing the Right Approach

- Depends on the specific objectives and characteristics of the financial time series data
- ARIMA models:
  - Forecasting future values
  - Accounting for autocorrelation and capturing underlying patterns
- Smoothing techniques:
  - Denoising the data and revealing the underlying trend or signal

## Combining Approaches

- ARIMA modelling and smoothing techniques can be combined
- Used in conjunction with other techniques (decomposition methods, machine learning algorithms)
- Helps gain deeper insights into financial time series data

## Financial time series smoothing

- Financial time series data often exhibit noise, irregularities, and fluctuations
- Smoothing techniques help reduce random variations and reveal underlying trends
- This presentation explores various smoothing methods used in financial time series analysis

## some data {visibility='hidden'}

```{r}
# Load required packages
library(quantmod)
library(TTR)
library(ggplot2)
library(dlm)
library(signal)
library(stats)

# Retrieve historical stock data for Apple Inc. (AAPL)
getSymbols("AAPL", from = "2015-01-01", to = "2020-12-31")

# Extract the adjusted closing prices
aapl_prices <- Cl(AAPL)
```



## Simple Moving Average (SMA)
- Basic smoothing technique that calculates the average of a fixed number of data points
- Formula: $SMA(t) = \frac{y(t) + y(t-1) + ... + y(t-n+1)}{n}$
- Widely used in technical analysis for identifying trends and generating trading signals
- Advantages: Easy to understand and implement, effective for removing high-frequency noise
- Limitations: Introduces a lag, sensitive to outliers, may distort underlying patterns

## Simple Moving Average (SMA) {.small}

```{r}
#| echo: true
# Simple Moving Average (SMA)
sma_20 <- SMA(aapl_prices, n = 20)
# Plot SMA
ggplot() +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(aapl_prices)), color = "black") +
  geom_line(aes(x = index(sma_20), y = as.numeric(sma_20)), color = "red") +
  labs(title = "Simple Moving Average (SMA)",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("black", "red"), labels = c("Price", "SMA")) +
  theme_minimal()
```

## Exponential Moving Average (EMA)

- Assigns exponentially decreasing weights to older data points
- Formula: $EMA(t) = \alpha \times y(t) + (1 - \alpha) \times EMA(t-1)$
- Commonly used in technical analysis, forecasting, and signal processing
- Advantages: Responds quickly to changes, less lag than SMA, less sensitive to outliers
- Limitations: Requires tuning the smoothing parameter ($\alpha$)

## Exponential Moving Average (EMA) {.small}
```{r}
#| echo: true
# Exponential Moving Average (EMA)
ema_20 <- EMA(aapl_prices, n = 20)
ggplot() +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(aapl_prices)), color = "black") +
  geom_line(aes(x = index(ema_20), y = as.numeric(ema_20)), color = "blue") +
  labs(title = "Exponential Moving Average (EMA)",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("black", "blue"), labels = c("Price", "EMA")) +
  theme_minimal()
```

## Weighted Moving Average (WMA)

- Assigns different weights to data points within the window
- Formula: $WMA(t) = \frac{w_1 \times y(t) + w_2 \times y(t-1) + ... + w_n \times y(t-n+1)}{w_1 + w_2 + ... + w_n}$
- Used in technical analysis, signal processing, and trend analysis
- Advantages: Allows for customized weighting schemes, can better capture underlying patterns
- Limitations: Requires careful selection of weights, inappropriate weights can distort the series

## Weighted Moving Average (WMA) {.small}

```{r}
#| echo: true
wma_custom <- WMA(aapl_prices, n = 5, wts = c(0.1, 0.2, 0.3, 0.2, 0.2))
ggplot() +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(aapl_prices)), color = "black") +
  geom_line(aes(x = index(wma_custom), y = as.numeric(wma_custom)), color = "green") +
  labs(title = "Weighted Moving Average (WMA)",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("black", "green"), labels = c("Price", "WMA")) +
  theme_minimal()
```

## Savitzky-Golay Filter

- Performs local polynomial regression on a moving window of data points
- Widely used in signal processing, spectroscopy, and financial time series analysis
- Advantages: Preserves data features (peaks and valleys), handles noisy data effectively
- Limitations: Computationally expensive, choice of polynomial order and window size affects results

## Savitzky-Golay Filter {.small}

```{r}
#| echo: true
sg_filter <- sgolayfilt(aapl_prices, p = 3, n = 21)
# Plot Savitzky-Golay Filter
ggplot() +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(aapl_prices)), color = "black") +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(sg_filter)), color = "purple") +
  labs(title = "Savitzky-Golay Filter",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("black", "purple"), labels = c("Price", "SG Filter")) +
  theme_minimal()
```

## Lowess (Locally Weighted Scatterplot Smoothing)

- Non-parametric regression technique that fits a low-degree polynomial to localized subsets of data
- Useful for identifying non-linear trends and patterns in financial time series data
- Advantages: Effective for handling non-linear relationships, robust to outliers, captures complex patterns
- Limitations: Computationally intensive, sensitive to the choice of smoothing parameters

## Lowess (Locally Weighted Scatterplot Smoothing) {.small}

```{r}
#| echo: true
# Lowess Smoothing
lowess_smooth <- lowess(aapl_prices)
# Plot Lowess Smoothing
ggplot() +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(aapl_prices)), color = "black") +
  geom_line(aes(x = index(aapl_prices), y = as.numeric(lowess_smooth$y)), color = "orange") +
  labs(title = "Lowess Smoothing",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("black", "orange"), labels = c("Price", "Lowess")) +
  theme_minimal()
```

## Kalman Filter

- Recursive algorithm that estimates the actual state of a dynamic system from noisy observations
- Widely used in finance for portfolio optimization, risk management, and forecasting
- Advantages: Optimal for linear systems with Gaussian noise, handles missing data, provides state estimates and uncertainties
- Limitations: Assumes linear system model and Gaussian noise, performance degrades for non-linear or non-Gaussian systems

## Kalman Filter {.small}

```{r}
```{r}
#| echo: true
# Apply the Kalman filter
s <- dlmSmooth(aapl_prices, dlmModPoly(1, dV = 15100, dW = 1470))

# Create a data frame for plotting
data_df <- data.frame(Date = index(aapl_prices),
                      Price = as.numeric(aapl_prices),
                      Kalman = as.numeric(dropFirst(s$s)))

# Plot the results using ggplot
data_df |> 
  ggplot(aes(x = Date)) +
  geom_line(aes(y = Price, color = "Price")) +
  geom_line(aes(y = Kalman, color = "Kalman")) +
  labs(title = "Apple Inc. (AAPL) Stock Prices",
       x = "Time",
       y = "Adjusted Close") +
  scale_color_manual(name = "Series", values = c("Price" = "black", "Kalman" = "blue")) +
  theme_minimal()
```

## Choosing the Right Smoothing Technique

- Depends on the characteristics of the financial time series data
- Consider the desired smoothing level and specific application or analysis goals
- Explore multiple smoothing methods and compare their performance
- Consider the trade-off between smoothing and preserving important features or patterns
- Excessive smoothing can lead to loss of valuable information
- Insufficient smoothing may fail to effectively remove unwanted noise

## Combining Smoothing Techniques

- Financial analysts and researchers may combine different smoothing techniques
- Advanced methods like wavelets or machine learning algorithms can be used
- Helps extract meaningful insights from complex financial time series data
